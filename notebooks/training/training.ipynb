{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcd4c7d-402f-4506-9a6f-13755860801e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23cd3796-d1a2-4af3-ad15-cbea85a10376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import torch\n",
    "import transformers\n",
    "import tokenizers\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "from llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from torch.utils.data import Dataset\n",
    "from llava.train.llava_trainer import LLaVATrainer\n",
    "from llava.train.train import DataArguments, TrainingArguments, ModelArguments, LazySupervisedDataset, DataCollatorForSupervisedDataset\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.model import *\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b9835-55e7-4c0a-9565-186a21d17abf",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c267fc-89b6-439e-861d-0a743621c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Return all of the Linear Layers in the neural network\n",
    "'''\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler']\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            # print(f\"{name} is Linear\")\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == -1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    \n",
    "    return list(lora_module_names)\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\n",
    "                                data_path=data_args.data_path,\n",
    "                                data_args=data_args)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset,\n",
    "                eval_dataset=None,\n",
    "                data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6487de2-5171-4ca2-8a94-6d40ac7580eb",
   "metadata": {},
   "source": [
    "# Define Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2816151-8152-4b31-b134-0de8d2bc52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Args\n",
    "model_name = 'liuhaotian/llava-v1.5-7b'\n",
    "attn_implementation = 'flash_attention_2'\n",
    "cache_dir = None\n",
    "torch_dtype = torch.bfloat16\n",
    "model_max_length=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4dd60f-1e73-478c-a130-a136de37b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Arguments\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path='liuhaotian/llava-v1.5-7b',\n",
    "    version='v1',\n",
    "    freeze_backbone=False,\n",
    "    tune_mm_mlp_adapter=False,\n",
    "    vision_tower='openai/clip-vit-large-patch14-336',\n",
    "    mm_vision_select_layer=-2,\n",
    "    pretrain_mm_mlp_adapter=None,\n",
    "    mm_projector_type='mlp2x_gelu',\n",
    "    mm_use_im_start_end=False,\n",
    "    mm_use_im_patch_token=False,\n",
    "    mm_patch_merge_type='flat',\n",
    "    mm_vision_select_feature='patch'\n",
    ")\n",
    "\n",
    "# Data Arguments\n",
    "data_args = DataArguments(\n",
    "    data_path='/home/devonperoutky/LLaVA/dataset/augmented/full_dataset.json',\n",
    "    lazy_preprocess=True, # ?????\n",
    "    is_multimodal=True,   # What does this do? False originally?????\n",
    "    image_folder='/home/devonperoutky/LLaVA/dataset/',\n",
    "    image_aspect_ratio='pad'\n",
    ")\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    group_by_modality_length=True,\n",
    "    bf16=True,\n",
    "    output_dir=\"./checkpoints/llava-v1.5-7b-run-1\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50000,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    tf32=True,\n",
    "    model_max_length=2048,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"wandb\",\n",
    "\n",
    "    # Lora Args\n",
    "    lora_enable=True,\n",
    "    mm_projector_lr=2e-05,\n",
    "    lora_r = 128,\n",
    "    lora_bias = \"none\",\n",
    "    lora_alpha = 256,\n",
    "    lora_dropout = .05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76857412-39b3-4d61-ab3b-c961fe1d853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards:   0%|                                                                                               | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:02<00:00, 31.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlavaLlamaForCausalLM(\n",
       "  (model): LlavaLlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (vision_tower): CLIPVisionTower()\n",
       "    (mm_projector): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    attn_implementation=attn_implementation,\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf36778a-d3e0-4c1a-a05b-33f2b8455728",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9087f1-0444-4103-b9ea-0dc9e3dc37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient checkpointing\n",
    "if hasattr(model, \"enable_input_require_grads\"):\n",
    "    model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f16322-a5fa-4f08-9eb5-29cd8f1369af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling Lora \n",
    "lora_config = LoraConfig(\n",
    "    r=training_args.lora_r,\n",
    "    lora_alpha=training_args.lora_alpha,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    lora_dropout=training_args.lora_dropout,\n",
    "    bias=training_args.lora_bias,\n",
    "    task_type=\"CASUAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f518d0f6-8338-4fea-b4e7-e4e99a9baabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_lib.default_conversation = conversation_lib.conv_templates[\"v1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe88d4ce-7cf0-45b0-8b0e-6a407c496840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.\n",
      "We are freezing the mm_mlp adapter\n",
      "We are not freezing the mm_mlp adapter\n"
     ]
    }
   ],
   "source": [
    "# What does this do? Initialize Vision Tower aka CLIP?\n",
    "model.get_model().initialize_vision_modules(\n",
    "    model_args=model_args,\n",
    "    fsdp=training_args.fsdp\n",
    ")\n",
    "vision_tower = model.get_vision_tower()\n",
    "vision_tower.to(dtype=torch_dtype, device=training_args.device)\n",
    "\n",
    "# Set Image Processor and multimodal\n",
    "data_args.image_processor = vision_tower.image_processor\n",
    "data_args.is_multimodal = True\n",
    "\n",
    "# Prepare model to dataset's preprocessing configuration\n",
    "model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "\n",
    "model.config.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter # False\n",
    "\n",
    "# We are NOT tuning\n",
    "if model_args.tune_mm_mlp_adapter:\n",
    "    print(\"Tuning the mm_mlp adapter\")\n",
    "    model.requires_grad_(False)\n",
    "    for p in model.get_model().mm_projector.parameters():\n",
    "        p.requires_grad = True\n",
    "else:\n",
    "    print(\"We are freezing the mm_mlp adapter\")\n",
    "\n",
    "model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "if training_args.freeze_mm_mlp_adapter:\n",
    "    print(\"We are freezing the mm_mlp adapter\")\n",
    "    for p in model.get_model().mm_projector.parameters():\n",
    "        p.requires_grad = False\n",
    "else:\n",
    "    print(\"We are not freezing the mm_mlp adapter\")        \n",
    "\n",
    "if training_args.bits in [4, 8]:\n",
    "    print(\"LITTLE BITS\")\n",
    "    model.get_model().mm_projector.to(dtype=compute_dtype, device=training_args.device)\n",
    "\n",
    "model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a676913-d3b1-4ddf-ae14-800809de48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_args)\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11ca15e-691d-4448-a0b5-912915e95aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaLlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaFlashAttention2(\n",
       "        (q_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (k_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (v_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (o_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=128, out_features=11008, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (up_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=128, out_features=11008, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (down_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=11008, out_features=128, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "        )\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       "  (vision_tower): CLIPVisionTower(\n",
       "    (vision_tower): CLIPVisionModel(\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(577, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mm_projector): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14e727-de09-4104-b54b-abb2402573c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LLaVATrainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc30b87-a1f5-49f4-9f57-9d4f35e24b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c36e0d-b152-491b-acfd-08826b43c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b152a29-bbbe-4ebd-8522-2430d09ec4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "313d6697-4bfe-4bbd-bda6-4f5cc49cd78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_module_parameters(module, parent_name=''):\n",
    "    \"\"\"\n",
    "    Recursively print a module's parameters and their 'requires_grad' status.\n",
    "\n",
    "    Args:\n",
    "    - module (nn.Module): The current module to print parameters from.\n",
    "    - parent_name (str): Name of the parent module, used for hierarchical naming.\n",
    "    \"\"\"\n",
    "    # If the module has children, we're going to call this function on them\n",
    "    if list(module.children()):\n",
    "        # Module name (type) for more readable output\n",
    "        module_name = module.__class__.__name__\n",
    "        current_name = f\"{parent_name}.{module_name}\" if parent_name else module_name\n",
    "\n",
    "        if isinstance(module, ModuleList):\n",
    "            layers = [(name, child) for name, child in module.named_children()]\n",
    "            print(f\"{len(layers)} {current_name} layers(s)\")\n",
    "            print(f\"{'-'*15} Start of Layer Structure {'-'*15}\")\n",
    "            child_name, child = layers[0]\n",
    "            print_module_parameters(child, \"\")\n",
    "            print(f\"{'-'*30} End of Layer Structure {'-'*15}\")\n",
    "\n",
    "            # for name, child in module.named_children():\n",
    "            #     print(name)\n",
    "            #     print(child)\n",
    "            #     # Construct child's full name by appending its name to the parent's name\n",
    "            #     child_name = f\"{current_name}.{name}\" if name else current_name\n",
    "            #     print_module_parameters(child, child_name)\n",
    "        else:\n",
    "            for name, child in module.named_children():\n",
    "                # Construct child's full name by appending its name to the parent's name\n",
    "                child_name = f\"{current_name}.{name}\" if name else current_name\n",
    "                print_module_parameters(child, child_name)\n",
    "    else:\n",
    "        # Leaf module, print its parameters\n",
    "        for name, param in module.named_parameters(recurse=False):\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            print(f\"{full_name}: {'Frozen' if not param.requires_grad else 'Trainable'}\")\n",
    "\n",
    "\n",
    "def output_layer(name, module, depth=0):\n",
    "    # print(name)\n",
    "    print(\"=\" * 30)\n",
    "    prefix = \" \" * depth\n",
    "    print(f\"{prefix} {name}\")\n",
    "\n",
    "    # Print out information about current layer\n",
    "\n",
    "    # For each module, output_layer\n",
    "    named_modules = [(n, m) for n, m in module.named_modules()]\n",
    "    if not named_modules or len(named_modules) < 1:\n",
    "        return\n",
    "\n",
    "    # print(named_modules[0])\n",
    "    # print(named_modules[1])\n",
    "    first = True\n",
    "    for n, m in named_modules:\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        \n",
    "        print(\"-\"*30)\n",
    "        print(f\"{n} is of type {m.__class__.__name__}\")\n",
    "        print(m.children())\n",
    "        # output_layer(n, m, depth+1)\n",
    "\n",
    "        # print(\"<---->\")\n",
    "        # for n1, m1 in m.named_modules():\n",
    "        #     print(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6be42eb3-478d-4fad-b882-c2f29701af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaLlamaModel.LlavaLlamaModel.embed_tokens.weight: Frozen\n",
      "32 LlavaLlamaModel.LlavaLlamaModel.layers.ModuleList layers(s)\n",
      "--------------- Start of Layer Structure ---------------\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.q_proj.Linear.base_layer.weight: Frozen\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.q_proj.Linear.lora_A.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.q_proj.Linear.lora_B.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.k_proj.Linear.base_layer.weight: Frozen\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.k_proj.Linear.lora_A.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.k_proj.Linear.lora_B.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.v_proj.Linear.base_layer.weight: Frozen\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.v_proj.Linear.lora_A.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.v_proj.Linear.lora_B.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.o_proj.Linear.base_layer.weight: Frozen\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.o_proj.Linear.lora_A.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.o_proj.Linear.lora_B.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.gate_proj.Linear.base_layer.weight: Frozen\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.gate_proj.Linear.lora_A.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.gate_proj.Linear.lora_B.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.up_proj.Linear.base_layer.weight: Frozen\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.up_proj.Linear.lora_A.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.up_proj.Linear.lora_B.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.down_proj.Linear.base_layer.weight: Frozen\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.down_proj.Linear.lora_A.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.down_proj.Linear.lora_B.ModuleDict.default.weight: Trainable\n",
      "LlamaDecoderLayer.input_layernorm.weight: Frozen\n",
      "LlamaDecoderLayer.post_attention_layernorm.weight: Frozen\n",
      "------------------------------ End of Layer Structure ---------------\n",
      "LlavaLlamaModel.LlavaLlamaModel.norm.weight: Frozen\n",
      "LlavaLlamaModel.LlavaLlamaModel.vision_tower.CLIPVisionTower.vision_tower.CLIPVisionModel.vision_model.CLIPVisionTransformer.embeddings.CLIPVisionEmbeddings.patch_embedding.weight: Frozen\n",
      "LlavaLlamaModel.LlavaLlamaModel.vision_tower.CLIPVisionTower.vision_tower.CLIPVisionModel.vision_model.CLIPVisionTransformer.embeddings.CLIPVisionEmbeddings.position_embedding.weight: Frozen\n",
      "LlavaLlamaModel.LlavaLlamaModel.vision_tower.CLIPVisionTower.vision_tower.CLIPVisionModel.vision_model.CLIPVisionTransformer.pre_layrnorm.weight: Frozen\n",
      "LlavaLlamaModel.LlavaLlamaModel.vision_tower.CLIPVisionTower.vision_tower.CLIPVisionModel.vision_model.CLIPVisionTransformer.pre_layrnorm.bias: Frozen\n",
      "24 LlavaLlamaModel.LlavaLlamaModel.vision_tower.CLIPVisionTower.vision_tower.CLIPVisionModel.vision_model.CLIPVisionTransformer.encoder.CLIPEncoder.layers.ModuleList layers(s)\n",
      "--------------- Start of Layer Structure ---------------\n",
      "CLIPEncoderLayer.self_attn.CLIPAttention.k_proj.weight: Frozen\n",
      "CLIPEncoderLayer.self_attn.CLIPAttention.k_proj.bias: Frozen\n",
      "CLIPEncoderLayer.self_attn.CLIPAttention.v_proj.weight: Frozen\n",
      "CLIPEncoderLayer.self_attn.CLIPAttention.v_proj.bias: Frozen\n",
      "CLIPEncoderLayer.self_attn.CLIPAttention.q_proj.weight: Frozen\n",
      "CLIPEncoderLayer.self_attn.CLIPAttention.q_proj.bias: Frozen\n",
      "CLIPEncoderLayer.self_attn.CLIPAttention.out_proj.weight: Frozen\n",
      "CLIPEncoderLayer.self_attn.CLIPAttention.out_proj.bias: Frozen\n",
      "CLIPEncoderLayer.layer_norm1.weight: Frozen\n",
      "CLIPEncoderLayer.layer_norm1.bias: Frozen\n",
      "CLIPEncoderLayer.mlp.CLIPMLP.fc1.weight: Frozen\n",
      "CLIPEncoderLayer.mlp.CLIPMLP.fc1.bias: Frozen\n",
      "CLIPEncoderLayer.mlp.CLIPMLP.fc2.weight: Frozen\n",
      "CLIPEncoderLayer.mlp.CLIPMLP.fc2.bias: Frozen\n",
      "CLIPEncoderLayer.layer_norm2.weight: Frozen\n",
      "CLIPEncoderLayer.layer_norm2.bias: Frozen\n",
      "------------------------------ End of Layer Structure ---------------\n",
      "LlavaLlamaModel.LlavaLlamaModel.vision_tower.CLIPVisionTower.vision_tower.CLIPVisionModel.vision_model.CLIPVisionTransformer.post_layernorm.weight: Frozen\n",
      "LlavaLlamaModel.LlavaLlamaModel.vision_tower.CLIPVisionTower.vision_tower.CLIPVisionModel.vision_model.CLIPVisionTransformer.post_layernorm.bias: Frozen\n",
      "LlavaLlamaModel.LlavaLlamaModel.mm_projector.Sequential.0.weight: Trainable\n",
      "LlavaLlamaModel.LlavaLlamaModel.mm_projector.Sequential.0.bias: Trainable\n",
      "LlavaLlamaModel.LlavaLlamaModel.mm_projector.Sequential.2.weight: Trainable\n",
      "LlavaLlamaModel.LlavaLlamaModel.mm_projector.Sequential.2.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "print_module_parameters(model.get_model(), \"LlavaLlamaModel\")\n",
    "# output_layer(\"LlavaLlamaModel\", model.get_model(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80fdd8-308a-40d0-ba94-2af27252ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.get_model().named_modules():\n",
    "    if not name or name == '' or name == 'layers':\n",
    "        continue\n",
    "\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    print(module.__class__.__name__)\n",
    "    print(f\"{module.__class__.__name__} aka {name}\")\n",
    "\n",
    "    for n, m in module.named_modules():\n",
    "        print(f\"{m.__class__.__name__} aka {n}\")\n",
    "        print(f\"{len([mod for mod in m.modules()]) -1} sub modules\")\n",
    "        print(f\"{len([mod for mod in m.parameters()])} parameters\")\n",
    "        # print(m)\n",
    "\n",
    "\n",
    "    # print(module)\n",
    "    # print(f\"{len([m for m in module.modules()]) -1} sub modules\")\n",
    "    # print([(p.shape, p.requires_grad) for p in module.parameters()])\n",
    "    \n",
    "    # for p in module.parameters():\n",
    "    #     print(p.shape)\n",
    "\n",
    "print(model.get_model().named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5e8ff-d955-4536-a23a-84a37bebaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_model().mm_projector.parameters()\n",
    "model.get_model().layers.parameters()\n",
    "model.get_model().layers.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76031800-6617-4647-a4a7-13761c46229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, m in model.get_model().named_modules():\n",
    "    if not list(m.children()):\n",
    "        pass\n",
    "    else:\n",
    "        print(n)\n",
    "        print(type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cdf4b-a4cc-45f9-b7b6-d813ead52513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "vicuna_tokenizer = LlamaTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "vicuna = LlamaForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddd6ec-2994-4372-905d-c3f24ab914a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embed_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb15830-4bb1-4ea0-adb6-4e6f5945b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496259de-0fd7-477a-8a99-3da93c934e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPConfig, CLIPModel\n",
    "\n",
    "# CLIP\n",
    "configuration = CLIPConfig()\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c9e47-6b30-4696-9d7a-ff9f666cb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd43ac-4e3e-4338-8825-f231811664ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
