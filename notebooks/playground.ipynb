{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41d14e87-2115-442c-8dc6-63c7687d7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from peft import PeftConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520c49d-eabe-47a1-a0fa-9265f7fb5cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model_name = \"lmsys/vicuna-13b-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693492c0-fca4-4f33-9f5f-041bd337ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f0ba416-176a-4636-b692-4404c9433e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d64acd-654c-4964-af35-19686193b01e",
   "metadata": {},
   "source": [
    "# Inference\n",
    "- [x] Test we can run inference on 13B\n",
    "- [x] Test we load a lora, trained on 7B, onto 13B\n",
    "- [ ] Build text-only QA dataset\n",
    "- [ ] Fine-tune 13B on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be477e-3a46-4f68-a3ea-56d6fabcee19",
   "metadata": {},
   "source": [
    "## Generic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2651aed0-549f-41c4-9298-805903b34d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> Hi, who are you?\\n\\nI'm a language model called Vicuna, and I was trained by Large Model Systems Organization (LMSYS) researchers.</s>\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Hi, who are you?\", return_tensors='pt').to(device)\n",
    "output_ids = model.generate(input_ids.input_ids)\n",
    "tokenizer.batch_decode(output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300df6d5-323a-483d-bb21-d6dc765e7710",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9053f98-0d84-4ff3-a90a-1040e07d864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_7b_lora = \"Charlie911/vicuna-7b-v1.5-lora-sharegpt-without-timedial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2e108ac-d688-4416-ba6d-ab75ec323029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='lmsys/vicuna-7b-v1.5', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=16, target_modules={'q_proj', 'v_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = PeftConfig.from_pretrained(test_7b_lora)\n",
    "model = peft.get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1adfa7b4-8961-46cd-a5a2-11e5a19cfcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 13,028,971,520 || trainable%: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2d951cd-b639-43cc-b875-88c9fc54e621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> Hi, how are you?\\n\\nI'm a new user here and I'm trying to learn more about the platform. I'm interested in creating a new post, but I'm not sure how to get started. Can you help me?\\n\\nThanks!</s>\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Hi, how are you?\", return_tensors='pt').to(device)\n",
    "output_ids = model.generate(input_ids.input_ids)\n",
    "tokenizer.batch_decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad30e4-b8e3-4315-913c-ca38e143e60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
