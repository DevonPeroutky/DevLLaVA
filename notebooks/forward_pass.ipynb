{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "514616bf-67cb-4d0d-9be6-524913a7c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-15 06:54:41,851] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 06:54:42.496510: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-15 06:54:42.550204: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-15 06:54:42.550236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-15 06:54:42.552583: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-15 06:54:42.561834: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import llava\n",
    "from torch.nn import functional as F\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.eval.run_llava import load_image\n",
    "from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "from transformers.generation.streamers import TextIteratorStreamer\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import LlamaConfig\n",
    "import importlib\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import time\n",
    "import subprocess\n",
    "from threading import Thread\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990444d-a698-4597-89d4-90e9d9be8227",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "\n",
    "deepspeed llava/train/train_mem.py \\\n",
    "    --lora_enable True --lora_r 256 --lora_alpha 512 --mm_projector_lr 2e-5 \\\n",
    "    --deepspeed ./scripts/zero3.json \\\n",
    "    --model_name_or_path liuhaotian/llava-v1.5-7b \\\n",
    "    --version v1 \\\n",
    "    --data_path /home/devonperoutky/LLaVA/dataset/augmented/train_dataset.json \\\n",
    "    --image_folder /home/devonperoutky/LLaVA/dataset/ \\\n",
    "    --vision_tower openai/clip-vit-large-patch14-336 \\\n",
    "    --mm_projector_type mlp2x_gelu \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --image_aspect_ratio pad \\\n",
    "    --group_by_modality_length True \\\n",
    "    --bf16 True \\\n",
    "    --output_dir ./checkpoints/llava-v1.5-7b-augmented-roastme-lora-train-rank256-2 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 50000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --report_to wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d0b66-dce3-49a3-83b7-8ab73b5916ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31061502-6745-4fc5-9461-2a34e75eab3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "training_args = None\n",
    "attn_implementation=\"flash_attention_2\"\n",
    "torch_dtype = torch.bfloat16\n",
    "bits = 16\n",
    "gradient_checkpointing=True\n",
    "lora_enable = True\n",
    "lora_r = 128\n",
    "lora_alpha = 256\n",
    "model_max_length=2048\n",
    "lora_dropout=.05\n",
    "lora_bias=None\n",
    "bf16 = True\n",
    "fp16 = False\n",
    "use_cache = False\n",
    "\n",
    "bnb_model_from_pretrained_args = {}\n",
    "if bits in [4, 8]:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_model_from_pretrained_args.update(dict(\n",
    "        device_map={\"\": training_args.device},\n",
    "        load_in_4bit=training_args.bits == 4,\n",
    "        load_in_8bit=training_args.bits == 8,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=training_args.bits == 4,\n",
    "            load_in_8bit=training_args.bits == 8,\n",
    "            llm_int8_skip_modules=[\"mm_projector\"],\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=training_args.double_quant,\n",
    "            bnb_4bit_quant_type=training_args.quant_type # {'fp4', 'nf4'}\n",
    "        )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb5f9e-b9d5-488e-a61e-f3d108137e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    cache_dir=training_args,\n",
    "    attn_implementation=attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    **bnb_model_from_pretrained_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87afeeb-cde7-4a3f-af5b-68a9bd9fb874",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if gradient_checkpointing:\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "if lora_enable:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=find_all_linear_names(model),\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=lora_bias,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    if bits == 16:\n",
    "        if bf16:\n",
    "            model.to(torch.bfloat16)\n",
    "        if fp16:\n",
    "            model.to(torch.float16)\n",
    "    rank0_print(\"Adding LoRA adapters...\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    cache_dir=cache_dir,\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6db619-79a5-449d-8ac8-cb42cce809bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca62233-8ac8-4a27-b438-2ce94d813a70",
   "metadata": {},
   "source": [
    "# Simple Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb6e95-31e0-435c-946d-6d5d48acc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_base = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951def9c-3446-4314-baa8-e0cea43500c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, model_base, model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18de7e-12cc-4382-a6d5-7b3ebdd56cef",
   "metadata": {},
   "source": [
    "# Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996186f3-6769-4b2f-8e22-437d2956f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"<image>\\nUSER: What's the content of the image?\\nASSISTANT:\"\n",
    "base_prompt = \"How would you insult this person?\"\n",
    "augmented_prompt = f'A chat between a curious human and mean-spirited, clever therapist. The therapist tries to insult the user in the most relevant, hurtful and clever way possible. USER: <image> {base_prompt} ASSISTANT:'\n",
    "test_image = \"https://preview.redd.it/yrdedweuk3ic1.jpeg?width=960&crop=smart&auto=webp&s=0ade9b61358296bfd98c43801cfe4b6dc8d2e243\"\n",
    "# model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_path = \"../merged_checkpoints/llava-augmented-roastme-v1-MERGE\"\n",
    "label_text = \"I spent a couple minutes thinking of a roast, but like your baby’s dad, I lost interest and don’t plan to think about you anymore.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f37025-5800-4452-a7a9-e1c109a08e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, prompt, top_p, temperature, max_new_tokens, loss):\n",
    "    # Configure conversational format to be Llava V1\n",
    "    conv_mode = \"llava_v1\"\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "\n",
    "    # Prepare prompt based on configuration?\n",
    "    inp = DEFAULT_IMAGE_TOKEN + '\\n' + prompt\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    # Load Image\n",
    "    image_data = load_image(str(image_path))\n",
    "    processed_image_input = image_processor.preprocess(image_data, return_tensors='pt')['pixel_values'].half().cuda()\n",
    "\n",
    "    images = [image_data]\n",
    "    image_sizes = [x.size for x in images]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "    print(prompt)\n",
    "    print(images_tensor.shape)\n",
    "\n",
    "    # Process prompt\n",
    "    input_ids = tokenizer_image_token(augmented_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "\n",
    "    print(input_ids.shape)\n",
    "    print(processed_image_input.shape)\n",
    "\n",
    "    if loss:\n",
    "        output = model.forward(input_ids=input_ids)\n",
    "        return output\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                images=processed_image_input,\n",
    "                image_sizes=image_sizes,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                num_beams=1,\n",
    "                top_p=top_p,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        return tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f1f31-f331-45b7-b87b-63968bc25784",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(test_image, prompt, 1.0, .2, 512, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc968b-dadd-4506-a314-51af3722e18b",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ac8e00-c9a1-4c57-a743-ab2277d45017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_templates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Configure conversational format to be Llava V1\u001b[39;00m\n\u001b[1;32m      2\u001b[0m conv_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m conv \u001b[38;5;241m=\u001b[39m \u001b[43mconv_templates\u001b[49m[conv_mode]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare prompt based on configuration?\u001b[39;00m\n\u001b[1;32m      6\u001b[0m inp \u001b[38;5;241m=\u001b[39m DEFAULT_IMAGE_TOKEN \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m base_prompt\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_templates' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load Image\n",
    "image_data = load_image(str(test_image))\n",
    "images = [image_data]\n",
    "image_sizes = [x.size for x in images]\n",
    "images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    ").to(model.device, dtype=torch.float16)\n",
    "processed_image_input = image_processor.preprocess(image_data, return_tensors='pt')['pixel_values'].half().cuda()\n",
    "\n",
    "\n",
    "# Process prompt\n",
    "print(augmented_prompt)\n",
    "input_ids = tokenizer_image_token(augmented_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "print(input_ids.shape)\n",
    "print(processed_image_input.shape)\n",
    "\n",
    "\n",
    "model.forward(input_ids, images=images_tensor, image_sizes=image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d235dd-fb46-4343-aba6-a49f9c20eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(xb, yb):\n",
    "    print(xb.shape)\n",
    "    print(yb.shape)\n",
    "\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = xb[..., :-1, :].contiguous()\n",
    "    shift_labels = yb[..., 1:].contiguous()\n",
    "    print(shift_logits.shape)\n",
    "    print(shift_labels.shape)\n",
    "    \n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    shift_logits = xb.view(-1, model.config.vocab_size)\n",
    "    shift_labels = yb.view(-1)\n",
    "    \n",
    "    # Enable model parallelism\n",
    "    shift_labels = shift_labels.to(shift_logits.device)\n",
    "    print(shift_logits.shape)\n",
    "    print(shift_labels.shape)\n",
    "    return loss_fct(shift_logits, shift_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfba79e-fc27-44df-8fd4-4b3f23b94f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_ids = input_ids\n",
    "preds = torch.empty(0, device=\"cuda\", dtype=torch.int64)\n",
    "alt_preds = torch.empty(0, device=\"cuda\", dtype=torch.int64)\n",
    "empty_tensor = torch.empty(0, 1)\n",
    "labels = tokenizer.encode(label_text, return_tensors='pt', padding=\"max_length\", max_length=curr_ids.shape[1]).cuda()\n",
    "\n",
    "# Remove start token\n",
    "# labels = labels[..., 1:].contiguous()\n",
    "\n",
    "for i in range(5):\n",
    "    (_input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels_embeds) = model.prepare_inputs_labels_for_multimodal(input_ids=input_ids, position_ids=None, attention_mask=None, past_key_values=None, labels=labels, images=images_tensor, image_sizes=image_sizes)\n",
    "    print(f'Input EMbeds: {inputs_embeds.shap\\e}')\n",
    "    print(f'Labels EMbeds: {labels_embeds.shape}')\n",
    "    output = model.forward(_input_ids, images=images_tensor, image_sizes=image_sizes, inputs_embeds=inputs_embeds)\n",
    "\n",
    "    final_layer = output.logits[:, -1, :]\n",
    "    idx_layer = output.logits[:, i, :]\n",
    "\n",
    "    print(compute_loss(output.logits, labels_embeds))\n",
    "    \n",
    "    predicted_token = F.softmax(final_layer).argmax().unsqueeze(dim=0)\n",
    "    alt_predicted_token = F.softmax(idx_layer).argmax().unsqueeze(dim=0)\n",
    "    \n",
    "    preds = torch.cat((preds, predicted_token), dim=0)\n",
    "    alt_preds = torch.cat((alt_preds, alt_predicted_token), dim=0)\n",
    "    curr_ids = torch.cat((input_ids, preds.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "print(tokenizer.batch_decode(preds))\n",
    "print(tokenizer.batch_decode(alt_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7160379-fed6-4506-a678-e43205069775",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenizer.encode(label, return_tensors='pt', padding=\"max_length\", max_length=input_ids.shape[1]).cuda()\n",
    "\n",
    "print(labels.shape)\n",
    "print(input_ids.shape)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12308bbc-a8f8-42b5-a33c-a7017c9df23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[..., 1:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01c6a9-f974-42c6-b1f8-42762a5d01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:, 1:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0702ae8-4bf2-42ef-ae77-2efb0cc16aeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m(input_ids, images\u001b[38;5;241m=\u001b[39mimages_tensor, image_sizes\u001b[38;5;241m=\u001b[39mimage_sizes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model(input_ids, images=images_tensor, image_sizes=image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb9217f-214f-4394-9266-fd6012207136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
