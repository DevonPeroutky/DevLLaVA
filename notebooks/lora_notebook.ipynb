{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8a070-9f29-4c81-a0a2-265d94328d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import llava\n",
    "from llava.model import *\n",
    "from torch.nn import functional as F\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.eval.run_llava import load_image\n",
    "from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "from transformers.generation.streamers import TextIteratorStreamer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from llava.serve.baristia_utils import  load_image_processor\n",
    "from peft import PeftModel, get_peft_model, PeftConfig\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import LlamaConfig\n",
    "import importlib\n",
    "from llava.serve.barista import LoraInferenceService\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import time\n",
    "import subprocess\n",
    "from threading import Thread\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336adb2a-e9a2-4a54-a23d-616daf3d5804",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50657f-9047-4139-a8d2-ac186d938184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"<image>\\nUSER: What's the content of the image?\\nASSISTANT:\"\n",
    "system_prompt = \"A chat between a curious human and mean-spirited, clever therapist. The therapist tries to insult the user in the most relevant, hurtful and clever way possible.\"\n",
    "base_prompt = \"How would you insult this person?\"\n",
    "prompt = f'{system_prompt} <image>\\nUSER: {base_prompt} ASSISTANT:'\n",
    "label = \"I spent a couple minutes thinking of a roast, but like your baby’s dad, I lost interest and don’t plan to think about you anymore.\"\n",
    "# test_image = \"https://preview.redd.it/yrdedweuk3ic1.jpeg?width=960&crop=smart&auto=webp&s=0ade9b61358296bfd98c43801cfe4b6dc8d2e243\"\n",
    "test_image = \"https://i.redd.it/8dnekc5w4nfa1.jpg\"\n",
    "# model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_path = \"../merged_checkpoints/llava-augmented-roastme-v1-MERGE\"\n",
    "image_data = load_image(str(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a5f34-6f4d-47e9-b236-b1672e462a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "lora_model_path = '../checkpoints/llava-v1.5-7b-augmented-roastme-lora-13000-4-epochs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b180182-9f5d-4ff0-8c25-069cc09cca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = reference_model_path\n",
    "load_8bit=False\n",
    "load_4bit=False\n",
    "device='cuda'\n",
    "device_map=\"auto\"\n",
    "use_flash_attn=False\n",
    "torch_dtype=torch.bfloat16\n",
    "kwargs = {\n",
    "    'torch_dtype': torch_dtype,\n",
    "    'device_map': device_map,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ed237-cdac-4afd-92ad-9b8f91959aa0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(inference_model, image_data: Image, prompt: str, system_prompt: str, top_p: float, temperature: float, max_new_tokens: int):\n",
    "    augmented_prompt = f'{system_prompt} USER: <image> {prompt} ASSISTANT:'\n",
    "    augmented_prompt_without_imagetag = f'{system_prompt} USER: <image> {prompt} ASSISTANT:'\n",
    "\n",
    "    print(f'Full Prompt: {augmented_prompt}')\n",
    "\n",
    "    # Load Image\n",
    "    processed_image_input = image_processor.preprocess(image_data, return_tensors='pt')['pixel_values'].to(inference_model.device, dtype=inference_model.dtype)\n",
    "\n",
    "    images = [image_data]\n",
    "    image_sizes = [x.size for x in images]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        inference_model.config\n",
    "    ).to(inference_model.device, dtype=inference_model.dtype)\n",
    "\n",
    "    print(augmented_prompt)\n",
    "    print(images_tensor.shape)\n",
    "\n",
    "    # Process prompt\n",
    "    input_ids = tokenizer_image_token(augmented_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "\n",
    "    print(input_ids.shape)\n",
    "    print(processed_image_input.shape)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = inference_model.generate(\n",
    "            input_ids,\n",
    "            images=processed_image_input,\n",
    "            image_sizes=image_sizes,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            num_beams=1,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    return (augmented_prompt, tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip())\n",
    "\n",
    "def load_lora_weights(base_model, lora_path):\n",
    "\n",
    "    # ???\n",
    "    token_num, token_dim = base_model.lm_head.out_features, base_model.lm_head.in_features\n",
    "    print(f'Token num: {token_num} (Vocab Size?)')\n",
    "    print(f'Token dim: {token_dim} (Hidden dimension size?)')\n",
    "    print(base_model.lm_head.weight.shape)\n",
    "    if base_model.lm_head.weight.shape[0] != token_num:\n",
    "        base_model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=base_model.device, dtype=base_model.dtype))\n",
    "        base_model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=base_model.device, dtype=base_model.dtype))\n",
    "    print(base_model.lm_head)\n",
    "\n",
    "    print('Loading additional LLaVA weights...')\n",
    "    if os.path.exists(os.path.join(lora_path, 'non_lora_trainables.bin')):\n",
    "        print(\"Non-trainable\")\n",
    "        non_lora_trainables = torch.load(os.path.join(lora_path, 'non_lora_trainables.bin'), map_location='cpu')\n",
    "    else:\n",
    "        print(\"Load from the Hub\")\n",
    "\n",
    "        def load_from_hf(repo_id, filename, subfolder=None):\n",
    "            cache_file = hf_hub_download(\n",
    "                repo_id=repo_id,\n",
    "                filename=filename,\n",
    "                subfolder=subfolder\n",
    "            )\n",
    "            return torch.load(cache_file, map_location='cpu')\n",
    "        non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\n",
    "\n",
    "    print(non_lora_trainables)\n",
    "    non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "    print(non_lora_trainables)\n",
    "    if any(k.startswith('model.model.') for k in non_lora_trainables):\n",
    "        non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "\n",
    "    print(f'Non Lora Trainables: {non_lora_trainables}')\n",
    "\n",
    "    # Load the lora? What is the difference between this and merge?\n",
    "    base_model.load_state_dict(non_lora_trainables, strict=False)\n",
    "    print(base_model)\n",
    "\n",
    "    print('Loading LoRA weights...')\n",
    "    base_model = PeftModel.from_pretrained(base_model, lora_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    print(base_model)\n",
    "\n",
    "    print('Merging LoRA weights...')\n",
    "    base_model = base_model.merge_and_unload()\n",
    "    print('Model is loaded...')\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf899c4e-82bf-4747-b3f6-5a48c9911e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_with_lora = load_lora_weights(model, lora_model_path)\n",
    "model_with_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a160870-a92c-483f-9ef1-e69b8fe13937",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = PeftConfig.from_pretrained(lora_model_path)\n",
    "model = PeftModel.from_pretrained(model=model, model_id=lora_model_path, adapter_name=\"test_lora\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.add_adapter(adapter_name=\"test_lora\", peft_config=lora_config)\n",
    "model.set_adapter(adapter_name=\"test_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3534e2-3da1-4b0f-9b16-a5d6e67d47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(adapter_name=\"test-lora\", peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4faaf2d-109c-49fa-9b41-842bb17b8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_adapters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05a650-9f11-4c6c-b30e-b6013c35edfb",
   "metadata": {},
   "source": [
    "# Simple (This works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a168d2b-d9aa-40e3-a170-3e38197b491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "# Model name\n",
    "model_name = get_model_name_from_path(reference_model_path)\n",
    "\n",
    "#Set Path to folder that contains adapter_config.json and the associated .bin files for the Peft model\n",
    "adapters_name = lora_model_path\n",
    "\n",
    "# Get PeftConfig from the finetuned Peft Model. This config file contains the path to the base model\n",
    "peft_config = PeftConfig.from_pretrained(adapters_name)\n",
    "\n",
    "# No vision tower solution\n",
    "lora_cfg_pretrained = LlavaConfig.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "# Load_pretrained_model\n",
    "tokenizer, base_model, image_processor, context_len = load_pretrained_model(peft_config.base_model_name_or_path, model_name=model_name, model_base=None, load_8bit=False, load_4bit=False)\n",
    "base_model.to(dtype=torch.bfloat16)\n",
    "\n",
    "# image_processor, context_len = load_image_processor(base_model, tokenizer, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1dc3a9-c212-4ca7-82e9-6ac37791f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(inference_model=base_model, image_data=image_data, prompt=base_prompt, system_prompt=system_prompt, top_p=.1, temperature=.8, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d14fdf-dd24-458a-8696-2e36abad3a76",
   "metadata": {},
   "source": [
    "# Load Lora (Basic, doesn't work with empty response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfbb41-9bcb-451f-b51f-eb1c042d5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Peft Model Id\n",
    "peft_model_id = lora_model_path\n",
    "\n",
    "#Set Path to folder that contains adapter_config.json and the associated .bin files for the Peft model\n",
    "adapters_name = lora_model_path\n",
    "\n",
    "# Get PeftConfig from the finetuned Peft Model. This config file contains the path to the base model\n",
    "peft_config = PeftConfig.from_pretrained(adapters_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31c780-27d0-4882-bc0c-bafa95cc11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Peft/Lora model\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53551f0-456a-4236-9398-163469f57e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(inference_model=model, image_data=image_data, prompt=base_prompt, system_prompt=system_prompt, top_p=.1, temperature=.8, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6cc6d-e2df-4780-9091-1a34e565e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "model.forward(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ef664-846d-44ad-8101-22e8c6d4fbbe",
   "metadata": {},
   "source": [
    "# Load Lora (THIS WORKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03117f7f-5ca4-498e-a550-0044a70c0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = lora_model_path\n",
    "# model_base = peft_config.base_model_name_or_path\n",
    "kwargs = {\n",
    "    'torch_dtype': torch_dtype,\n",
    "    'device_map': device_map,\n",
    "}\n",
    "\n",
    "# Model name\n",
    "model_name = get_model_name_from_path(reference_model_path)\n",
    "\n",
    "peft_model_id = lora_model_path\n",
    "\n",
    "#Set Path to folder that contains adapter_config.json and the associated .bin files for the Peft model\n",
    "adapters_name = lora_model_path\n",
    "\n",
    "# Get PeftConfig from the finetuned Peft Model. This config file contains the path to the base model\n",
    "peft_config = PeftConfig.from_pretrained(adapters_name)\n",
    "\n",
    "lora_cfg_pretrained = LlavaConfig.from_pretrained(lora_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45055d3-ba28-485b-b8aa-c79dffdf2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, use_fast=False)\n",
    "model = LlavaLlamaForCausalLM.from_pretrained(peft_config.base_model_name_or_path, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847dd38-fc06-4471-a86a-41ad67132bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor, context_len = load_image_processor(model, tokenizer, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54a960-2ebe-4017-93e3-8f0febb6db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_num, token_dim = model.lm_head.out_features, model.lm_head.in_features\n",
    "print(f'Token num: {token_num} (Vocab Size?)')\n",
    "print(f'Token dim: {token_dim} (Hidden dimension size?)')\n",
    "print(model.lm_head.weight.shape)\n",
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5465d1-2e44-4f92-9a20-fb583ce0646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_lora_trainables = torch.load(os.path.join(lora_model_path, 'non_lora_trainables.bin'), map_location='cpu')\n",
    "non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "if any(k.startswith('model.model.') for k in non_lora_trainables):\n",
    "    non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a773c-adae-4bce-890a-5a4690a1abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lora? What is the difference between this and merge?\n",
    "model.load_state_dict(non_lora_trainables, strict=False)\n",
    "                                                               \n",
    "print('Loading LoRA weights...')\n",
    "model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14366c1c-71c5-4133-b35c-fe6afe5b57ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict(inference_model=model, image_data=image_data, prompt=base_prompt, system_prompt=system_prompt, top_p=.5, temperature=.8, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6abc3-38d5-4a63-9c8c-68291d9bfb75",
   "metadata": {},
   "source": [
    "# What about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb69047-439c-472d-973e-841e8a4b0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.unload()\n",
    "predict(inference_model=model, image_data=image_data, prompt=base_prompt, system_prompt=system_prompt, top_p=.5, temperature=.8, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b817f2f-92f6-4eb6-993b-7e86b9f55371",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd290823-3b43-441f-97b8-37a5531f0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(inference_model=model, image_data=image_data, prompt=base_prompt, system_prompt=system_prompt, top_p=.5, temperature=.8, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc778bb-43e4-43e1-9fa3-4eadf8f0f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6567c47d-36e4-4f69-bd7c-0da8b814513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lora? What is the difference between this and merge?\n",
    "model.load_state_dict(non_lora_trainables, strict=False)\n",
    "                                                               \n",
    "print('Loading LoRA weights...')\n",
    "model = PeftModel.from_pretrained(model, lora_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30fd56-875a-40ba-b3e0-8b264b5da612",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(inference_model=model, image_data=image_data, prompt=base_prompt, system_prompt=system_prompt, top_p=.5, temperature=.8, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fba5fb-c6a6-4bdb-8af0-99a8880fd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm = model.unload()\n",
    "hey = [k for k, v in bm.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c354b-0549-4fcc-81db-a95d3d47a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bm.load_state_dict(non_lora_trainables, strict=False)\n",
    "yo = [k for k, v in bm.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12f84c-a828-495e-9bac-c77fcbee3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hey) == len(yo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c89aa-79c5-4d8f-98b7-6087e9b4fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8546ba3-909a-467e-bc3b-cac2f0a1b262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
