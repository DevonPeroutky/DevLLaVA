{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab6879ea-f5f1-4cb7-badf-7870c3dacb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import transformers\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, CLIPVisionModel,AutoProcessor\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from transformers import AutoTokenizer\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536776c-1094-4817-94ad-e28f27347449",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"liuhaotian/llava-v1.5-7b\"\n",
    "base_prompt = \"What is in this image?\"\n",
    "image_url = \"https://buffer.com/cdn-cgi/image/w=1000,fit=contain,q=90,f=auto/library/content/images/size/w600/2023/10/free-images.jpg\"\n",
    "\n",
    "# Load image from url\n",
    "response = requests.get(image_url)\n",
    "image_data = Image.open(BytesIO(response.content))\n",
    "image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799194b2-8399-4292-84f2-3d488de0b186",
   "metadata": {},
   "source": [
    "# Instantiate Model and its encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4992d49-4b0b-4488-bca2-ad7884f7a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32a8abe-c303-4a22-9007-832d0029d1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                              | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model with the simplest possible settings.\n",
    "llava_model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype = torch.float16, # So it can fit on my a100 better\n",
    ").to(device)\n",
    "\n",
    "# Text Encoder\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# Visual Encoder\n",
    "vision_tower = llava_model.get_vision_tower()\n",
    "vision_tower.load_model(device_map='auto')\n",
    "image_processor = vision_tower.image_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8bc150-c281-4cc6-a7da-8b4f80dd0a0e",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c0a42-ecc7-4037-b2df-a2bb85e4a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're curious about what this function does\n",
    "def prepare_prompt_into_expected_format(prompt):\n",
    "    conv = conv_templates[\"llava_v1\"].copy()\n",
    "\n",
    "    # just one turn, always prepend image token\n",
    "    inp = DEFAULT_IMAGE_TOKEN + '\\n' + prompt\n",
    "    \n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c7ef7-fe09-449b-b14e-3d9ad1ea476b",
   "metadata": {},
   "source": [
    "## Generate Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6eb30-d0f5-40a1-af20-a77e9cf01d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn user prompt into conversation format that vicuna (the LLM piece of LLaVA) is expecting.\n",
    "prompt = prepare_prompt_into_expected_format(base_prompt)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d9b9f-6ea0-4f6d-bfcb-f76df30b5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the textual embeddings for the prompt. Exactly the same if this were an LLM\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574c965-70fd-4a92-9cde-bd1d057ecf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the visual embeddings for the corresponding image\n",
    "image_encodings = image_processor.preprocess(image_data, return_tensors='pt')['pixel_values'].half().cuda()\n",
    "image_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4512f-1f25-4d13-9110-ba84ef399076",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = llava_model.generate(input_ids, image_encodings)\n",
    "tokenizer.batch_decode(output_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703344a-366c-44d1-af8b-ced6ffc51274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "So this a very important step, as we have used the visual encoder to process an image into a tensor representation, which we will be able to project into a shared space, with the textual embeddings, to generate our output from. This visual encoder for LLaVA, along with many other multimodal models, is CLIP. \n",
    "\n",
    "## Clip\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c500037-8005-49cf-bac4-781aed54c069",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "Now if you have any experience with Neural Networks, you're probably scratching your head right now, because you would expect these embeddings to share the same dimensions. For the unacclimated, deep learning is heavily based on matrix multiplication. These matrix multiplications can be heavily optimized to take advantage of the speed of GPUs. However, they require the inputs to be compatible dimensionally. Attempting to multiply matrices Tensors with mismatched dimensions will lead to the dreaded\n",
    "```\n",
    "RuntimeError: stack expects each tensor to be equal size, but got [3, 224, 224] at entry 0 and [3, 224, 336] at entry 3\n",
    "```\n",
    "So what is going on?\n",
    "\n",
    "We'll this is where the projection matrix (the main piece of the multimodal puzzle) comes into play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d9e3e-6c4b-4fc5-ac62-c2933565b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt = \"Hello, how are you doing today?\"\n",
    "\n",
    "# Convert string prompt into tokens\n",
    "tokens = tokenizer.tokenize(prompt, return_tensors='pt')\n",
    "print(tokens)\n",
    "\n",
    "# Convert tokens to input_ids, by getting the index of that token in the vocabulary.\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)\n",
    "\n",
    "# Single step (aka the normal way)\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2903f7-38dd-46e9-84c0-122803b9ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The squeeze(0) is to convert \n",
    "output_ids = llava_model.generate(input_ids)\n",
    "tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbcff9-e468-449b-a820-d054c9b930b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e94454-8efe-4e10-a924-2e02d83ddbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_model.get_model().vision_tower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba61d6-57aa-478b-9c83-7dd2aed9924d",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a94a2-f54f-4240-85c8-0088fe394f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai/clip-vit-large-patch14-336\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b00190-575f-4243-adf0-30be1dc20cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs\n",
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13700dbe-101e-4955-9ff0-973cb5d00722",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841758f-07a4-4962-ae9f-63e15cfc5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c102b-ce7a-4b97-ba32-863f989397f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e67b2-f891-43f6-a2df-97e20c626441",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_model.get_vision_tower()(images=inputs.pixel_values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ef89e-2757-424b-8ddf-d59b5d9502ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7c571-feab-4d97-8df1-9f7479d16ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = llava_model.get_model().embed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb973c-7f58-4062-a08e-6bebd5713d41",
   "metadata": {},
   "source": [
    "# Basic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2518da5-4a8b-413c-b9ba-949988b54a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hi', ',', '▁how', '▁are', '▁you', '?', '▁', '<0x0A>']\n",
      "tensor([[    1,  6324, 29892,   920,   526,   366, 29973, 29871,    13]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s> Hi, how are you? \\n']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_prompt = \"Hi, how are you? \\n\"\n",
    "tokens = tokenizer(base_prompt, return_tensors='pt').to(device)\n",
    "print(tokenizer.tokenize(base_prompt))\n",
    "print(tokens.input_ids)\n",
    "\n",
    "tokenizer.batch_decode(tokens.input_ids, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5adad7cc-eebb-4daf-a3a4-599d142418f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.5471e-03, -3.8147e-03,  1.7242e-03,  ..., -8.7891e-03,\n",
       "           2.5024e-03, -2.4719e-03],\n",
       "         [ 2.5024e-02, -1.0254e-02, -1.3550e-02,  ...,  4.1199e-03,\n",
       "          -4.3945e-03, -2.1851e-02],\n",
       "         [-3.4142e-04, -3.7537e-03, -6.9580e-03,  ...,  7.9727e-04,\n",
       "          -3.5095e-03,  4.8523e-03],\n",
       "         ...,\n",
       "         [-1.2512e-02,  1.4709e-02, -5.4321e-03,  ...,  6.8359e-03,\n",
       "          -1.6861e-03, -5.6458e-04],\n",
       "         [-1.2283e-03,  1.3199e-03, -1.2695e-02,  ...,  2.5940e-03,\n",
       "          -1.1902e-03, -5.3406e-03],\n",
       "         [-1.6212e-04, -2.1648e-04,  7.1335e-04,  ...,  3.5667e-04,\n",
       "           4.3297e-04, -7.0572e-05]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = llava_model.get_model().embed_tokens\n",
    "embedding_layer(tokens.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4acef83b-d93c-4445-be57-991feaf14986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10, 32000])\n",
      "torch.Size([1, 32000])\n",
      "tensor([29915], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_1038535/1631927741.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predicted_token_id = F.softmax(final_prediction_layer).argmax().unsqueeze(dim=0)\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "layers = llava_model.forward(input_ids).logits\n",
    "print(layers.shape)\n",
    "\n",
    "final_prediction_layer = layers[:, -1, :]\n",
    "print(final_prediction_layer.shape)\n",
    "\n",
    "predicted_token_id = F.softmax(final_prediction_layer).argmax().unsqueeze(dim=0)\n",
    "print(predicted_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34e2fc50-2764-474a-8ebb-b79d32992dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_1038535/4257912497.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predicted_token = F.softmax(final_prediction_layer).argmax().unsqueeze(dim=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([29915], device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token = F.softmax(final_prediction_layer).argmax().unsqueeze(dim=0)\n",
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "883277d2-061c-45a9-baba-d74dd2629542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0323b7ff-a1bf-4334-aecf-f2a72d27c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat((iputs, predicted_token.unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e2a6ca4-61c4-4d8d-97c9-15c58f6e3411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> Hi, how are you? \\n I']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73909db6-9758-4c3b-9b4b-4f0e0fd30bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  6324, 29892,   920,   526,   366, 29973, 29871,    13]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e7d685df-e689-4f59-8b0d-01f90fb0cb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  6324, 29892,   920,   526,   366, 29973, 29871,    13,   306]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "be5f0648-d5b6-4f17-b3e6-ab572b874d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29915"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e631e01d-f4fe-4f35-8853-2e8b73182c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hi', ',', '▁how', '▁are', '▁you', '?', '▁', '<0x0A>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_1038535/3859289862.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predicted_token_id = F.softmax(final_prediction_layer).argmax().unsqueeze(dim=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<s> Hi, how are you? \\n I'm good, thanks for asking.  How about you?</s>\"]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_prompt = \"Hi, how are you? \\n\"\n",
    "tokens = tokenizer(base_prompt, return_tensors='pt').to(device)\n",
    "print(tokenizer.tokenize(base_prompt))\n",
    "input_ids = tokens.input_ids\n",
    "\n",
    "tokenizer.batch_decode(tokens.input_ids, skip_special_tokens=False)\n",
    "\n",
    "eos_token_id = 2\n",
    "predicted_token_id = torch.tensor([[0]])\n",
    "\n",
    "# While we haven't predicted stop token\n",
    "while predicted_token_id.item() != eos_token_id:\n",
    "\n",
    "\toutput = llava_model.forward(input_ids)\n",
    "\tlayers = output.logits\n",
    "\tfinal_prediction_layer = layers[:, -1, :]\n",
    "\tpredicted_token_id = F.softmax(final_prediction_layer).argmax().unsqueeze(dim=0)\n",
    "\t\n",
    "\t# Concate predicted_token_id to existing sequence of token_ids\n",
    "\tinput_ids = torch.cat((input_ids, predicted_token_id.unsqueeze(1)), dim=1)\n",
    "\t\n",
    "\n",
    "# Response\n",
    "tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2b8b2-49c2-475a-8a8c-0586ba86948f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
