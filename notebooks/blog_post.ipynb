{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab6879ea-f5f1-4cb7-badf-7870c3dacb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from transformers import AutoTokenizer\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d536776c-1094-4817-94ad-e28f27347449",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is in this image?\"\n",
    "image_url = \"https://buffer.com/cdn-cgi/image/w=1000,fit=contain,q=90,f=auto/library/content/images/size/w600/2023/10/free-images.jpg\"\n",
    "\n",
    "# Load image from url\n",
    "response = requests.get(image_url)\n",
    "image_data = Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799194b2-8399-4292-84f2-3d488de0b186",
   "metadata": {},
   "source": [
    "# Instantiate Model and its encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32a8abe-c303-4a22-9007-832d0029d1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                              | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"liuhaotian/llava-v1.5-7b\"\n",
    "\n",
    "# Instantiate model with the simplest possible settings.\n",
    "model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype = torch.float16, # So it can fit on my a100 better\n",
    ")\n",
    "\n",
    "# Text Encoder\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# Visual Encoder\n",
    "vision_tower = model.get_vision_tower()\n",
    "vision_tower.load_model(device_map='auto')\n",
    "image_processor = vision_tower.image_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8bc150-c281-4cc6-a7da-8b4f80dd0a0e",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c7ef7-fe09-449b-b14e-3d9ad1ea476b",
   "metadata": {},
   "source": [
    "## Generate Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ab6eb30-d0f5-40a1-af20-a77e9cf01d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nWhat is in this image? ASSISTANT:\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn user prompt into conversation format that vicuna (the LLM piece of LLaVA) is expecting.\n",
    "prompt = prepare_prompt_into_expected_format(prompt)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d2d9b9f-6ea0-4f6d-bfcb-f76df30b5389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the textual embeddings for the prompt. Exactly the same if this were an LLM\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d04c0a42-ecc7-4037-b2df-a2bb85e4a9de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# If you're curious about what this function does\n",
    "def prepare_prompt_into_expected_format(prompt):\n",
    "    conv = conv_templates[\"llava_v1\"].copy()\n",
    "\n",
    "    # just one turn, always prepend image token\n",
    "    inp = DEFAULT_IMAGE_TOKEN + '\\n' + prompt\n",
    "    \n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9574c965-70fd-4a92-9cde-bd1d057ecf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 336, 336])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the visual embeddings for the corresponding image\n",
    "image_encodings = image_processor.preprocess(image_data, return_tensors='pt')['pixel_values'].half().cuda()\n",
    "image_encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703344a-366c-44d1-af8b-ced6ffc51274",
   "metadata": {},
   "source": [
    "So this a very important step, as we have used the visual encoder to process an image into a tensor representation, which we will be able to project into a shared space, with the textual embeddings, to generate our output from. This visual encoder for LLaVA, along with many other multimodal models, is CLIP. \n",
    "\n",
    "## Clip\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c500037-8005-49cf-bac4-781aed54c069",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "Now if you have any experience with Neural Networks, you're probably scratching your head right now, because you would expect these embeddings to share the same dimensions. For the unacclimated, deep learning is heavily based on matrix multiplication. These matrix multiplications can be heavily optimized to take advantage of the speed of GPUs. However, they require the inputs to be compatible dimensionally. Attempting to multiply matrices Tensors with mismatched dimensions will lead to the dreaded\n",
    "```\n",
    "RuntimeError: stack expects each tensor to be equal size, but got [3, 224, 224] at entry 0 and [3, 224, 336] at entry 3\n",
    "```\n",
    "So what is going on?\n",
    "\n",
    "We'll this is where the projection matrix (the main piece of the multimodal puzzle) comes into play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4992d49-4b0b-4488-bca2-ad7884f7a725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaLlamaForCausalLM(\n",
       "  (model): LlavaLlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (vision_tower): CLIPVisionTower(\n",
       "      (vision_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(577, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mm_projector): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f13d9e3e-6c4b-4fc5-ac62-c2933565b940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"liuhaotian/llava-v1.5-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "input_ids = tokenizer(\"Hello, how are you?\", return_tensors='pt').input_ids\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b2903f7-38dd-46e9-84c0-122803b9ab32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The squeeze(0) is to convert \n",
    "tokenizer.decode(input_ids.squeeze(0), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff90734a-6521-4029-ab46-bbb3b6d66b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153165a2-3c84-435c-b947-220f75207f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
