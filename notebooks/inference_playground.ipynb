{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c8ffd-c436-4c2f-89f6-6c66c49b169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import llava\n",
    "from torch.nn import functional as F\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.eval.run_llava import load_image\n",
    "from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "from transformers.generation.streamers import TextIteratorStreamer\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import LlamaConfig\n",
    "import importlib\n",
    "from llava.serve.barista import LoraInferenceService\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import time\n",
    "import subprocess\n",
    "from threading import Thread\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512fe605-21e4-416b-bda5-9c0f00d762c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2b126-ec91-4414-b141-8f233e236fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"<image>\\nUSER: What's the content of the image?\\nASSISTANT:\"\n",
    "system_prompt = \"A chat between a curious human and mean-spirited, clever therapist. The therapist tries to insult the user in the most relevant, hurtful and clever way possible.\"\n",
    "base_prompt = \"How would you insult this person?\"\n",
    "prompt = f'{system_prompt} <image>\\nUSER: {base_prompt} ASSISTANT:'\n",
    "label = \"I spent a couple minutes thinking of a roast, but like your baby’s dad, I lost interest and don’t plan to think about you anymore.\"\n",
    "# test_image = \"https://preview.redd.it/yrdedweuk3ic1.jpeg?width=960&crop=smart&auto=webp&s=0ade9b61358296bfd98c43801cfe4b6dc8d2e243\"\n",
    "test_image = \"https://i.redd.it/8dnekc5w4nfa1.jpg\"\n",
    "# model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_path = \"../merged_checkpoints/llava-augmented-roastme-v1-MERGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2fb1f-416e-49c2-a2aa-83cde1aca535",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\"liuhaotian/llava-v1.5-7b\", model_name=\"llava-v1.5-7b\", model_base=None, load_8bit=False, load_4bit=False)\n",
    "model.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a62215-5a20-4754-82ae-07529742b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(image_path, prompt):\n",
    "    # Configure conversational format to be Llava V1\n",
    "    conv_mode = \"llava_v1\"\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "\n",
    "    # Prepare prompt based on configuration?\n",
    "    inp = DEFAULT_IMAGE_TOKEN + '\\n' + prompt\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    # Load Image\n",
    "    image_data = load_image(str(image_path))\n",
    "\n",
    "    images = [image_data]\n",
    "    image_sizes = [x.size for x in images]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "    print(prompt)\n",
    "    return input_ids, images_tensor, image_sizes\n",
    "    \n",
    "def prepare_label_ids(input_ids, label):\n",
    "    return tokenizer.encode(label, return_tensors='pt', padding=\"max_length\", max_length=input_ids.shape[1]).cuda()\n",
    "\n",
    "def generate(image_path, prompt, top_p, temperature, max_new_tokens):\n",
    "    input_ids, images_tensor, image_sizes = prepare_inputs(image_path, prompt)    \n",
    "\n",
    "    print(input_ids.shape)\n",
    "    print(label_input_ids.shape)\n",
    "    print(images_tensor.shape)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            num_beams=1,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    return tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def forward(image_path, prompt, top_p, temperature, max_new_tokens, label):\n",
    "    input_ids, images_tensor, image_sizes = prepare_inputs(image_path, prompt) \n",
    "    label_input_ids = tokenizer.encode(label, return_tensors='pt', padding=\"max_length\", max_length=input_ids.shape[1]).cuda() if label else None\n",
    "    \n",
    "    print(input_ids.shape)\n",
    "    print(images_tensor.shape)    \n",
    "    \n",
    "    return model.forward(input_ids, images=images_tensor, use_cache=True, image_sizes=image_sizes, labels=label_input_ids)\n",
    "\n",
    "def the_call(image_path, prompt, top_p, temperature, max_new_tokens, label):\n",
    "    input_ids, images_tensor, image_sizes = prepare_inputs(image_path, prompt) \n",
    "    label_input_ids = tokenizer.encode(label, return_tensors='pt', padding=\"max_length\", max_length=input_ids.shape[1]).cuda()\n",
    "    \n",
    "    return model(input_ids=input_ids, images=images_tensor, use_cache=True, image_sizes=image_sizes, labels=label_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36682cb-f573-4f72-b39a-b8b07e36545e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate(test_image, prompt, 1.0, .2, 512)\n",
    "forward(test_image, prompt, 1.0, .2, 512, None)\n",
    "# call_outputs = the_call(test_image, prompt, 1.0, .2, 512, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7caa87c-bf55-4969-9804-d64dbbaec72a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forward_outputs.logits.shape[1]\n",
    "len([n for n, m in model.named_modules()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924c82e-a67d-41fc-b059-7d0d8a838d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_logit_layer = forward_outputs.logits[:,-1,:]\n",
    "max_token_prob = F.softmax(final_logit_layer).argmax()\n",
    "print(max_token_prob.shape)\n",
    "prediction = tokenizer.batch_decode(max_token_prob.unsqueeze(0), skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(f'Prediction {prediction} vs. {label} has loss: {forward_outputs.loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bd6ee-1197-4e51-a9a8-2c6f6d95e537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for logits, labels in zip([forward_outputs.logits, call_outputs.logits], [\"I\", \"YOU\", label]):\n",
    "    print(logits.shape)\n",
    "    loss = None\n",
    "    input_ids, images_tensor, image_sizes = prepare_inputs(test_image, prompt) \n",
    "    label_input_ids = tokenizer.encode(label, return_tensors='pt', padding=\"max_length\", max_length=input_ids.shape[1]).cuda()\n",
    "    config = LlamaConfig()\n",
    "    \n",
    "    print(input_ids.shape)\n",
    "    print(label_input_ids.shape)\n",
    "    print(\"1---\")\n",
    "    \n",
    "    if label_input_ids is not None:\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        # shift_logits = logits.contiguous()\n",
    "        shift_labels = label_input_ids[..., 1:].contiguous()\n",
    "        print(shift_logits.shape)\n",
    "        print(shift_labels.shape)\n",
    "        print(\"2-----\")\n",
    "        \n",
    "        # Is 625 the sequence length?????\n",
    "\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        shift_logits = shift_logits.view(-1, config.vocab_size)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        \n",
    "        # Enable model parallelism\n",
    "        shift_labels = shift_labels.to(shift_logits.device)\n",
    "        print(shift_logits.shape)\n",
    "        print(shift_labels.shape)\n",
    "        loss = loss_fct(shift_logits, shift_labels)\n",
    "        \n",
    "    print(f'Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a808a3c-0109-4989-9f2e-881e5f49f8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = LlamaConfig()\n",
    "output_attentions = config.output_attentions\n",
    "output_hidden_states = config.output_hidden_states\n",
    "return_dict = config.use_return_dict\n",
    "attention_mask = None\n",
    "position_ids = None\n",
    "past_key_values = None\n",
    "inputs_embeds = None\n",
    "use_cache = True\n",
    "cache_position = None\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377dc10-8ccc-4539-87d3-0d877ad8fff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "tokenizer.batch_decode(test, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b56701-8133-4405-95f2-43f5e594cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, images_tensor, image_sizes = prepare_inputs(test_image, prompt)\n",
    "label_input_ids = tokenizer.encode(label, return_tensors='pt', padding=\"max_length\", max_length=input_ids.shape[1]).cuda()\n",
    "\n",
    "(_input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = model.prepare_inputs_labels_for_multimodal(input_ids=input_ids, position_ids=None, attention_mask=None, past_key_values=None, labels=label_input_ids, images=images_tensor, image_sizes=image_sizes)\n",
    "\n",
    "outputs = model.forward(input_ids, position_ids=None, attention_mask=None, past_key_values=None, labels=label_input_ids, images=images_tensor, image_sizes=image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b998e74-8100-4437-99c4-08b77f0e24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "for i in range(logits.shape[1]):\n",
    "    logit_layer = logits[:,i ,:]\n",
    "    max_token_prob = F.softmax(logit_layer).argmax()\n",
    "    prediction = tokenizer.batch_decode(max_token_prob.unsqueeze(0), skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(f'Index {i} -> {prediction[0]}')\n",
    "\n",
    "    \n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # shift_logits = logits.contiguous()\n",
    "    shift_labels = label_input_ids[..., 1:].contiguous()\n",
    "    print(shift_logits.shape)\n",
    "    print(shift_labels.shape)\n",
    "    print(\"2-----\")\n",
    "    \n",
    "    #     # Is 625 the sequence length?????\n",
    "\n",
    "    #     # Flatten the tokens\n",
    "    #     loss_fct = CrossEntropyLoss()\n",
    "    #     shift_logits = shift_logits.view(-1, config.vocab_size)\n",
    "    #     shift_labels = shift_labels.view(-1)\n",
    "        \n",
    "    #     # Enable model parallelism\n",
    "    #     shift_labels = shift_labels.to(shift_logits.device)\n",
    "    #     print(shift_logits.shape)\n",
    "    #     print(shift_labels.shape)\n",
    "    #     loss = loss_fct(shift_logits, shift_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b7979-6320-40fa-b550-ec94abceb8d6",
   "metadata": {},
   "source": [
    "# Generate Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45467bdf-4fb2-4dd5-b7c2-33eea423dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic generate\n",
    "def generate(image_path, prompt, top_p, temperature, max_new_tokens):\n",
    "    input_ids, images_tensor, image_sizes = prepare_inputs(image_path, prompt)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            num_beams=1,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    return tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "generate(test_image, prompt, .21, .1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bda336-2474-4ccf-8589-f0a758fd95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, image_path, prompt, top_p, temperature, max_new_tokens, label):\n",
    "    input_ids, images_tensor, image_sizes = prepare_inputs(image_path, prompt) \n",
    "    label_input_ids = tokenizer.encode(label, return_tensors='pt', padding=\"max_length\", max_length=input_ids.shape[1]).cuda() if label else None\n",
    "    \n",
    "    print(input_ids.shape)\n",
    "    print(images_tensor.shape)    \n",
    "    \n",
    "    return model.forward(input_ids, images=images_tensor, use_cache=True, image_sizes=image_sizes, labels=label_input_ids)\n",
    "\n",
    "\n",
    "# Variables\n",
    "prompt = \"How would you insult this person?\"\n",
    "augmented_prompt = \"A chat between a curious human and mean-spirited, clever therapist. The therapist tries to insult the user in the most relevant, hurtful and clever way possible. <image>\\nUSER: How would you insult this person? ASSISTANT:\"\n",
    "label = \"I spent a couple minutes thinking of a roast, but like your baby’s dad, I lost interest and don’t plan to think about you anymore.\"\n",
    "predicted_token = None\n",
    "\n",
    "# Process prompt, image, and labels\n",
    "input_ids, images_tensor, image_sizes = prepare_inputs(test_image, prompt) \n",
    "label_input_ids = tokenizer.encode(label, return_tensors='pt', padding=\"max_length\", max_length=input_ids.shape[1]).cuda()\n",
    "curr_prompt_ids = input_ids\n",
    "\n",
    "for token in label_input_ids:\n",
    "    print(f'Current Prompt: {curr_prompt_ids}')\n",
    "\n",
    "    # Forward\n",
    "    outputs = model.forward(curr_prompt_ids, images=images_tensor, image_sizes=image_sizes)\n",
    "\n",
    "    # Grab final logits layer for prediction\n",
    "    final_logits_layer = outputs.logits[:, :-1, :]\n",
    "    print(f'Final Logits Layer: {final_logits_layer.shape}')\n",
    "\n",
    "    # Prediction\n",
    "    predicted_prob = F.softmax(final_logits_layer, dim=1).argmax()\n",
    "    print(f'Predict Token: {predicted_prob}')\n",
    "    print(f'Predict Token: {predicted_prob.unsqueeze(0)}')\n",
    "    print(f'Prediction: {tokenizer.batch_decode(predicted_prob.unsqueeze(0), skip_special_tokens=True, clean_up_tokenization_spaces=False)}')\n",
    "\n",
    "    # Iterate\n",
    "    curr_prompt_ids += predicted_prob\n",
    "\n",
    "    # Calculate loss?\n",
    "    del outputs\n",
    "    break\n",
    "\n",
    "outputs = model.forward(input_ids, images=images_tensor, image_sizes=image_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1353c-1211-4cc2-9351-c88dd752e60f",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9749350-e447-4440-8700-421e8766493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c8095-d8a5-47bd-afa6-bc33b181d8f6",
   "metadata": {},
   "source": [
    "# Matrix Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5e202-4dea-4e30-9c93-3a24fa3fb29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "\n",
    "# Shift logits because we need logits to predict the next character\n",
    "logits_for_predictions = logits[..., :-1, :]\n",
    "yb = labels[..., 1:]\n",
    "assert yb.shape == logits_for_predictions.shape[:2]\n",
    "\n",
    "# View?\n",
    "shift_logits = logits_for_predictions.view(-1, config.vocab_size)\n",
    "shift_labels = yb.view(-1)\n",
    "print(shift_logits.shape)\n",
    "print(shift_labels.shape)\n",
    "\n",
    "# Enable model parallelism\n",
    "shift_labels = shift_labels.to(shift_logits.device)\n",
    "loss_fct(shift_logits, shift_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8462f-111e-4639-9676-f3581e116636",
   "metadata": {},
   "source": [
    "# Individually Calculated Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916700b-f4e4-4053-9e0c-08628b4c73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "count = 0\n",
    "for i in range(logits.shape[1]):    \n",
    "    logit_layer = logits[:, i, :].contiguous()\n",
    "\n",
    "    # Prediction\n",
    "    max_token_prob = F.softmax(logit_layer).argmax()\n",
    "    prediction = tokenizer.batch_decode(max_token_prob.unsqueeze(0), skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    # Label\n",
    "    label_id = labels[0][i].unsqueeze(0)\n",
    "    if label_id.item() == -100:\n",
    "        break\n",
    "        \n",
    "    y = tokenizer.batch_decode(label_id, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    loss = loss_fct(logit_layer, label_id)\n",
    "    print(f'{i} --> Prediction {prediction} vs {y} w/Loss: {loss}')\n",
    "    \n",
    "    total_loss += loss\n",
    "    count += 1\n",
    "\n",
    "print(total_loss / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1986a7-696b-4fd8-bdd3-db12b6f952a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_logits[-2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d927ea-86a1-4775-af45-3be66eac20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f62f5f-e266-4f65-bc1e-1530e56423a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.empty((4, 64))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1be511-6402-403f-afa0-5773d3e17b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230b705-af14-4767-8ddf-7452d2485d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "lora_model_path = '../checkpoints/llava-v1.5-7b-augmented-roastme-lora-train-8-epochs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fdd984-2786-42ae-a48d-23d434f076d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_service = LoraInferenceService(model_path=reference_model_path, load_8bit=False, load_4bit=False)\n",
    "lora_service.model.device\n",
    "\n",
    "# lora_service.load_lora_weights('../checkpoints/llava-v1.5-7b-augmented-roastme-lora-train-8-epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f2a2c-8744-4276-b23d-dc26a9e9fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???\n",
    "# lora_service.tokenizer.pad_token = \"[PAD]\"\n",
    "# lora_service.tokenizer.padding_side = \"left\"\n",
    "# lora_service.model.half().cuda()\n",
    "# lora_service.model = lora_service.unload_lora_weights()\n",
    "\n",
    "# Load Image\n",
    "image_data = load_image(str(test_image))\n",
    "\n",
    "lora_service.predict(image_data=image_data, prompt=base_prompt, system_prompt=system_prompt, top_p=.1, temperature=.8, max_new_tokens=512, lora_model_path=lora_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7822ec5e-2693-4589-aa65-359eafc785a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'llava-v1.5-7b-augmented-roastme-lora-train-8-epochs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths = lora_model_path.split(\"/\")\n",
    "if model_paths[-1].startswith('checkpoint-'):\n",
    "    model_name = model_paths[-2] + \"_\" + model_paths[-1]\n",
    "else:\n",
    "    print('here')\n",
    "    model_name = model_paths[-1]\n",
    "\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc7bec-8ddd-49a4-8e28-b85cbddf5318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
