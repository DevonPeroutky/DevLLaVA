{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcd4c7d-402f-4506-9a6f-13755860801e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23cd3796-d1a2-4af3-ad15-cbea85a10376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import torch\n",
    "import transformers\n",
    "import tokenizers\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "from llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from torch.utils.data import Dataset\n",
    "from llava.train.llava_trainer import LLaVATrainer\n",
    "from llava.train.train import DataArguments, TrainingArguments, ModelArguments, LazySupervisedDataset, DataCollatorForSupervisedDataset\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.model import *\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b9835-55e7-4c0a-9565-186a21d17abf",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95c267fc-89b6-439e-861d-0a743621c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Return all of the Linear Layers in the neural network\n",
    "'''\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler']\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            # print(f\"{name} is Linear\")\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == -1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    \n",
    "    return list(lora_module_names)\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = LazySupervisedDataset(tokenizer=tokenizer,\n",
    "                                data_path=data_args.data_path,\n",
    "                                data_args=data_args)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset,\n",
    "                eval_dataset=None,\n",
    "                data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6487de2-5171-4ca2-8a94-6d40ac7580eb",
   "metadata": {},
   "source": [
    "# Define Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2816151-8152-4b31-b134-0de8d2bc52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Args\n",
    "model_name = 'liuhaotian/llava-v1.5-7b'\n",
    "attn_implementation = 'flash_attention_2'\n",
    "cache_dir = None\n",
    "torch_dtype = torch.bfloat16\n",
    "model_max_length=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f4dd60f-1e73-478c-a130-a136de37b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Arguments\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path='liuhaotian/llava-v1.5-7b',\n",
    "    version='v1',\n",
    "    freeze_backbone=False,\n",
    "    tune_mm_mlp_adapter=False,\n",
    "    vision_tower='openai/clip-vit-large-patch14-336',\n",
    "    mm_vision_select_layer=-2,\n",
    "    pretrain_mm_mlp_adapter=None,\n",
    "    mm_projector_type='mlp2x_gelu',\n",
    "    mm_use_im_start_end=False,\n",
    "    mm_use_im_patch_token=False,\n",
    "    mm_patch_merge_type='flat',\n",
    "    mm_vision_select_feature='patch'\n",
    ")\n",
    "\n",
    "# Data Arguments\n",
    "data_args = DataArguments(\n",
    "    data_path='/home/devonperoutky/LLaVA/dataset/augmented/full_dataset.json',\n",
    "    lazy_preprocess=True, # ?????\n",
    "    is_multimodal=True,   # What does this do? False originally?????\n",
    "    image_folder='/home/devonperoutky/LLaVA/dataset/',\n",
    "    image_aspect_ratio='pad'\n",
    ")\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    group_by_modality_length=True,\n",
    "    bf16=True,\n",
    "    output_dir=\"./checkpoints/llava-v1.5-7b-run-1\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50000,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    tf32=True,\n",
    "    model_max_length=2048,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"wandb\",\n",
    "\n",
    "    # Lora Args\n",
    "    lora_enable=True,\n",
    "    mm_projector_lr=2e-05,\n",
    "    lora_r = 128,\n",
    "    lora_bias = \"none\",\n",
    "    lora_alpha = 256,\n",
    "    lora_dropout = .05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76857412-39b3-4d61-ab3b-c961fe1d853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards:  50%|███████████████████████████████████████████▌                                           | 1/2 [00:29<00:29, 29.12s/it]"
     ]
    }
   ],
   "source": [
    "model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    attn_implementation=attn_implementation,\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36778a-d3e0-4c1a-a05b-33f2b8455728",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9087f1-0444-4103-b9ea-0dc9e3dc37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient checkpointing\n",
    "if hasattr(model, \"enable_input_require_grads\"):\n",
    "    model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f16322-a5fa-4f08-9eb5-29cd8f1369af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling Lora \n",
    "lora_config = LoraConfig(\n",
    "    r=training_args.lora_r,\n",
    "    lora_alpha=training_args.lora_alpha,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    lora_dropout=training_args.lora_dropout,\n",
    "    bias=training_args.lora_bias,\n",
    "    task_type=\"CASUAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518d0f6-8338-4fea-b4e7-e4e99a9baabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_lib.default_conversation = conversation_lib.conv_templates[\"v1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88d4ce-7cf0-45b0-8b0e-6a407c496840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this do? Initialize Vision Tower aka CLIP?\n",
    "model.get_model().initialize_vision_modules(\n",
    "    model_args=model_args,\n",
    "    fsdp=training_args.fsdp\n",
    ")\n",
    "vision_tower = model.get_vision_tower()\n",
    "vision_tower.to(dtype=torch_dtype, device=training_args.device)\n",
    "\n",
    "# Set Image Processor and multimodal\n",
    "data_args.image_processor = vision_tower.image_processor\n",
    "data_args.is_multimodal = True\n",
    "\n",
    "# Prepare model to dataset's preprocessing configuration\n",
    "model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "\n",
    "model.config.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter # False\n",
    "\n",
    "# We are NOT tuning\n",
    "if model_args.tune_mm_mlp_adapter:\n",
    "    print(\"Tuning the mm_mlp adapter\")\n",
    "    model.requires_grad_(False)\n",
    "    for p in model.get_model().mm_projector.parameters():\n",
    "        p.requires_grad = True\n",
    "else:\n",
    "    print(\"We are not tuning the mm_mlp adapter\")\n",
    "\n",
    "model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "if training_args.freeze_mm_mlp_adapter:\n",
    "    print(\"We are freezing the mm_mlp adapter\")\n",
    "    for p in model.get_model().mm_projector.parameters():\n",
    "        p.requires_grad = False\n",
    "else:\n",
    "    print(\"We are not freezing the mm_mlp adapter\")        \n",
    "\n",
    "if training_args.bits in [4, 8]:\n",
    "    print(\"LITTLE BITS\")\n",
    "    model.get_model().mm_projector.to(dtype=compute_dtype, device=training_args.device)\n",
    "\n",
    "model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a676913-d3b1-4ddf-ae14-800809de48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_args)\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ca15e-691d-4448-a0b5-912915e95aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14e727-de09-4104-b54b-abb2402573c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LLaVATrainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc30b87-a1f5-49f4-9f57-9d4f35e24b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c36e0d-b152-491b-acfd-08826b43c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b152a29-bbbe-4ebd-8522-2430d09ec4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "313d6697-4bfe-4bbd-bda6-4f5cc49cd78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_module_parameters(module, parent_name=''):\n",
    "    \"\"\"\n",
    "    Recursively print a module's parameters and their 'requires_grad' status.\n",
    "\n",
    "    Args:\n",
    "    - module (nn.Module): The current module to print parameters from.\n",
    "    - parent_name (str): Name of the parent module, used for hierarchical naming.\n",
    "    \"\"\"\n",
    "    # If the module has children, we're going to call this function on them\n",
    "    if list(module.children()):\n",
    "        # Module name (type) for more readable output\n",
    "        module_name = module.__class__.__name__\n",
    "        current_name = f\"{parent_name}.{module_name}\" if parent_name else module_name\n",
    "\n",
    "        if isinstance(module, ModuleList):\n",
    "            layers = [(name, child) for name, child in module.named_children()]\n",
    "            print(f\"{len(layers)} {current_name} layers(s)\")\n",
    "            print(f\"{'-'*15} Start of Layer Structure {'-'*15}\")\n",
    "            child_name, child = layers[0]\n",
    "            print_module_parameters(child, \"\")\n",
    "            print(f\"{'-'*30} End of Layer Structure {'-'*15}\")\n",
    "\n",
    "            # for name, child in module.named_children():\n",
    "            #     print(name)\n",
    "            #     print(child)\n",
    "            #     # Construct child's full name by appending its name to the parent's name\n",
    "            #     child_name = f\"{current_name}.{name}\" if name else current_name\n",
    "            #     print_module_parameters(child, child_name)\n",
    "        else:\n",
    "            for name, child in module.named_children():\n",
    "                # Construct child's full name by appending its name to the parent's name\n",
    "                child_name = f\"{current_name}.{name}\" if name else current_name\n",
    "                print_module_parameters(child, child_name)\n",
    "    else:\n",
    "        # Leaf module, print its parameters\n",
    "        for name, param in module.named_parameters(recurse=False):\n",
    "            full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "            print(f\"{full_name}: {'Frozen' if not param.requires_grad else 'Trainable'}\")\n",
    "\n",
    "\n",
    "def output_layer(name, module, depth=0):\n",
    "    # print(name)\n",
    "    print(\"=\" * 30)\n",
    "    prefix = \" \" * depth\n",
    "    print(f\"{prefix} {name}\")\n",
    "\n",
    "    # Print out information about current layer\n",
    "\n",
    "    # For each module, output_layer\n",
    "    named_modules = [(n, m) for n, m in module.named_modules()]\n",
    "    if not named_modules or len(named_modules) < 1:\n",
    "        return\n",
    "\n",
    "    # print(named_modules[0])\n",
    "    # print(named_modules[1])\n",
    "    first = True\n",
    "    for n, m in named_modules:\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        \n",
    "        print(\"-\"*30)\n",
    "        print(f\"{n} is of type {m.__class__.__name__}\")\n",
    "        print(m.children())\n",
    "        # output_layer(n, m, depth+1)\n",
    "\n",
    "        # print(\"<---->\")\n",
    "        # for n1, m1 in m.named_modules():\n",
    "        #     print(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6be42eb3-478d-4fad-b882-c2f29701af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaLlamaModel.LlavaLlamaModel.embed_tokens.weight: Trainable\n",
      "32 LlavaLlamaModel.LlavaLlamaModel.layers.ModuleList layers(s)\n",
      "--------------- Start of Layer Structure ---------------\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.q_proj.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.k_proj.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.v_proj.weight: Trainable\n",
      "LlamaDecoderLayer.self_attn.LlamaFlashAttention2.o_proj.weight: Trainable\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.gate_proj.weight: Trainable\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.up_proj.weight: Trainable\n",
      "LlamaDecoderLayer.mlp.LlamaMLP.down_proj.weight: Trainable\n",
      "LlamaDecoderLayer.input_layernorm.weight: Trainable\n",
      "LlamaDecoderLayer.post_attention_layernorm.weight: Trainable\n",
      "------------------------------ End of Layer Structure ---------------\n",
      "LlavaLlamaModel.LlavaLlamaModel.norm.weight: Trainable\n",
      "LlavaLlamaModel.LlavaLlamaModel.mm_projector.Sequential.0.weight: Trainable\n",
      "LlavaLlamaModel.LlavaLlamaModel.mm_projector.Sequential.0.bias: Trainable\n",
      "LlavaLlamaModel.LlavaLlamaModel.mm_projector.Sequential.2.weight: Trainable\n",
      "LlavaLlamaModel.LlavaLlamaModel.mm_projector.Sequential.2.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "print_module_parameters(model.get_model(), \"LlavaLlamaModel\")\n",
    "# output_layer(\"LlavaLlamaModel\", model.get_model(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80fdd8-308a-40d0-ba94-2af27252ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.get_model().named_modules():\n",
    "    if not name or name == '' or name == 'layers':\n",
    "        continue\n",
    "\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    print(module.__class__.__name__)\n",
    "    print(f\"{module.__class__.__name__} aka {name}\")\n",
    "\n",
    "    for n, m in module.named_modules():\n",
    "        print(f\"{m.__class__.__name__} aka {n}\")\n",
    "        print(f\"{len([mod for mod in m.modules()]) -1} sub modules\")\n",
    "        print(f\"{len([mod for mod in m.parameters()])} parameters\")\n",
    "        # print(m)\n",
    "\n",
    "\n",
    "    # print(module)\n",
    "    # print(f\"{len([m for m in module.modules()]) -1} sub modules\")\n",
    "    # print([(p.shape, p.requires_grad) for p in module.parameters()])\n",
    "    \n",
    "    # for p in module.parameters():\n",
    "    #     print(p.shape)\n",
    "\n",
    "print(model.get_model().named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e5e8ff-d955-4536-a23a-84a37bebaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_model().mm_projector.parameters()\n",
    "model.get_model().layers.parameters()\n",
    "model.get_model().layers.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76031800-6617-4647-a4a7-13761c46229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, m in model.get_model().named_modules():\n",
    "    if not list(m.children()):\n",
    "        pass\n",
    "    else:\n",
    "        print(n)\n",
    "        print(type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cdf4b-a4cc-45f9-b7b6-d813ead52513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "vicuna_tokenizer = LlamaTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "vicuna = LlamaForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddd6ec-2994-4372-905d-c3f24ab914a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embed_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb15830-4bb1-4ea0-adb6-4e6f5945b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496259de-0fd7-477a-8a99-3da93c934e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPConfig, CLIPModel\n",
    "\n",
    "# CLIP\n",
    "configuration = CLIPConfig()\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c9e47-6b30-4696-9d7a-ff9f666cb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd43ac-4e3e-4338-8825-f231811664ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
